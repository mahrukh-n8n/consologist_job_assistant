---
phase: 02-scraping-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/content/search-scraper.js
  - src/utils/job-parser.js
autonomous: true

must_haves:
  truths:
    - "Calling scrapeSearchPage() on a live Upwork search results URL returns an array with at least one job object"
    - "Every returned job object contains job_id, title, and url — no undefined values"
    - "job_id is extracted from the tilde-notation in the job card URL, not invented or guessed"
    - "The scraper handles zero results (empty feed) without throwing"
    - "The module exports a single named function that the content script and service worker can import"
  artifacts:
    - path: "src/content/search-scraper.js"
      provides: "scrapeSearchPage() — reads the current DOM and returns [{job_id, title, url}]"
      exports: ["scrapeSearchPage"]
    - path: "src/utils/job-parser.js"
      provides: "extractJobId(url) — parses tilde-notation job ID from any Upwork job URL"
      exports: ["extractJobId"]
  key_links:
    - from: "src/content/search-scraper.js"
      to: "src/utils/job-parser.js"
      via: "import { extractJobId }"
      pattern: "extractJobId"
    - from: "src/content/upwork-content.js"
      to: "src/content/search-scraper.js"
      via: "import { scrapeSearchPage }"
      pattern: "scrapeSearchPage"
---

<objective>
Build the search page scraper module that reads job cards from Upwork search result pages and returns structured data arrays.

Purpose: Phase 3 webhook push and CSV export both require scraped job arrays as input. This is the data source for all downstream phases when the user is on a search results page.
Output: src/content/search-scraper.js and src/utils/job-parser.js
</objective>

<execution_context>
@C:/Users/Glorvax/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Glorvax/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
</context>

## Field Name Mapping

All output field names are case-sensitive and must match the reference project exactly.

| DOM Element / Source | Output Field | Type | Notes |
|---|---|---|---|
| Job card link href (`/jobs/Title_~{id}`) | `job_id` | string | Extract via regex: `/_~([^?/]+)/` |
| Job card title text | `title` | string | `.trim()` whitespace |
| Job card link href (full resolved URL) | `url` | string | Absolute URL including origin |

<tasks>

<task type="auto">
  <name>Task 1: URL parser utility</name>
  <files>src/utils/job-parser.js</files>
  <action>
Create src/utils/job-parser.js with the following exported function:

```js
/**
 * Extracts the Upwork job ID from a tilde-notation job URL.
 * Upwork URL pattern: /jobs/Title-words_~{alphanumeric-id}/
 * Also handles apply URLs: /apply/Title_~{id}
 *
 * @param {string} url - Absolute or relative Upwork job URL
 * @returns {string|null} - The job ID string, or null if not found
 */
export function extractJobId(url) {
  if (!url) return null;
  const match = url.match(/_~([a-zA-Z0-9]+)/);
  return match ? match[1] : null;
}
```

Do NOT add any other logic. Keep this a pure string utility — no DOM access, no fetch calls.
  </action>
  <verify>Open browser console on any Upwork job URL and run the regex manually: `/jobs/Some-Job-Title_~01234abcd567ef89/`.match(/_~([a-zA-Z0-9]+)/) should return ["_~01234abcd567ef89", "01234abcd567ef89"]</verify>
  <done>extractJobId('/jobs/Some-Title_~abc123/') returns 'abc123'. extractJobId(null) returns null. extractJobId('/search/jobs/') returns null.</done>
</task>

<task type="auto">
  <name>Task 2: Search page scraper module</name>
  <files>src/content/search-scraper.js</files>
  <action>
Create src/content/search-scraper.js. This module runs inside a content script context (has full DOM access, no Node.js APIs).

**What to scrape:** Upwork search results render job cards in a feed. Each card is a list item or article element containing the job title link and metadata.

**Selector strategy — use the first selector that matches, fall back to the next:**

For job card containers (try in order):
1. `[data-test="job-tile"]`
2. `section.air3-card-section`
3. `article.job-tile`
4. `.job-tile`

For the title link inside each card (try in order):
1. `[data-test="job-title"] a`
2. `h2.job-title a`
3. `h2 a[href*="/jobs/"]`
4. `a[href*="/jobs/"]`

**Implementation:**

```js
import { extractJobId } from '../utils/job-parser.js';

/**
 * Scrapes job cards from the current Upwork search results page.
 * Returns an empty array if no cards are found — does not throw.
 *
 * @returns {Array<{job_id: string, title: string, url: string}>}
 */
export function scrapeSearchPage() {
  const cardSelectors = [
    '[data-test="job-tile"]',
    'section.air3-card-section',
    'article.job-tile',
    '.job-tile',
  ];

  const linkSelectors = [
    '[data-test="job-title"] a',
    'h2.job-title a',
    'h2 a[href*="/jobs/"]',
    'a[href*="/jobs/"]',
  ];

  // Find card container selector that actually matches
  let cards = [];
  for (const sel of cardSelectors) {
    const found = document.querySelectorAll(sel);
    if (found.length > 0) {
      cards = Array.from(found);
      break;
    }
  }

  const jobs = [];

  for (const card of cards) {
    let anchor = null;
    for (const sel of linkSelectors) {
      anchor = card.querySelector(sel);
      if (anchor) break;
    }

    if (!anchor) continue;

    const href = anchor.getAttribute('href') || '';
    const absoluteUrl = href.startsWith('http')
      ? href
      : `https://www.upwork.com${href}`;

    const job_id = extractJobId(absoluteUrl);
    const title = (anchor.textContent || '').trim();

    if (!job_id || !title) continue;

    jobs.push({ job_id, title, url: absoluteUrl });
  }

  return jobs;
}
```

Add a console.debug line at the end of the function (before return) for diagnostics:
`console.debug('[upwork-ext] search scrape:', jobs.length, 'jobs found');`
  </action>
  <verify>Load a Chrome extension with this content script on https://www.upwork.com/nx/search/jobs/ and run in the console: `scrapeSearchPage()`. Confirm array is returned, each element has job_id (not null), title (non-empty), url (starts with https://www.upwork.com).</verify>
  <done>scrapeSearchPage() on a loaded search results page returns an array of job objects. Each object has job_id (string matching tilde-notation ID), title (non-empty string), url (absolute Upwork URL). Empty page returns []. No exceptions thrown.</done>
</task>

</tasks>

<verification>
1. Load extension with manifest pointing content script at `https://www.upwork.com/*`
2. Navigate to `https://www.upwork.com/nx/search/jobs/`
3. Open DevTools console, confirm `[upwork-ext] search scrape: N jobs found` appears
4. Run `scrapeSearchPage()` in console, inspect one result: `{ job_id: "...", title: "...", url: "https://www.upwork.com/jobs/..." }`
5. Confirm job_id matches the `_~{id}` segment from the url field
6. Confirm no errors or undefined values
</verification>

<success_criteria>
- src/utils/job-parser.js exists and exports extractJobId
- src/content/search-scraper.js exists and exports scrapeSearchPage
- scrapeSearchPage() returns array of {job_id, title, url} with no undefined fields
- job_id is always extracted from URL tilde-notation, never null on valid job cards
- Empty or non-search pages return [] without throwing
- Field names are exactly: job_id, title, url (case-sensitive, snake_case)
</success_criteria>

<output>
After completion, create `.planning/phases/02-scraping-engine/02-01-SUMMARY.md` following the summary template.
</output>
